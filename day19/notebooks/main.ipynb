{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf93263a",
   "metadata": {},
   "source": [
    "# Day 19: Model Evaluation Metrics\n",
    "## Comprehensive Forecast Assessment Framework\n",
    "\n",
    "**Objective:** Master quantitative evaluation metrics for time series forecasts. Implement and compare MAE, RMSE, MAPE, SMAPE, MDA, and directional accuracy to select optimal forecasting models.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Accuracy metrics**: MAE, RMSE, MAPE - measuring prediction error\n",
    "- **Financial metrics**: MDA, directional accuracy - trading perspective\n",
    "- **Robustness**: Understanding metric sensitivity to outliers\n",
    "- **Scale-dependence**: When to use percentage vs absolute metrics\n",
    "- **Metric selection**: Choosing appropriate metrics for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from scipy import stats\n",
    "import gc\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv('./data/gold_prices.csv', parse_dates=['Date'])\n",
    "except:\n",
    "    try:\n",
    "        df = pd.read_csv('../data/gold_prices.csv', parse_dates=['Date'])\n",
    "    except:\n",
    "        df = pd.read_csv('/home/tads/Work/TADSPROJ/30-days-of-tsa/day19/data/gold_prices.csv', \n",
    "                        parse_dates=['Date'])\n",
    "\n",
    "# Clean data\n",
    "if 'Price' not in df.columns:\n",
    "    df = df.rename(columns={'Adj Close': 'Price'})\n",
    "df = df.drop_duplicates(subset=['Date']).sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Data loaded: {len(df)} observations\")\n",
    "print(f\"  Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "print(f\"  Price range: ${df['Price'].min():.2f} to ${df['Price'].max():.2f}\")\n",
    "\n",
    "# Train-test split (80/20)\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size].copy()\n",
    "test_df = df[train_size:].copy()\n",
    "\n",
    "print(f\"\\nTrain-Test Split:\")\n",
    "print(f\"  Train: {len(train_df)} obs | {train_df['Date'].min().date()} to {train_df['Date'].max().date()}\")\n",
    "print(f\"  Test:  {len(test_df)} obs | {test_df['Date'].min().date()} to {test_df['Date'].max().date()}\")\n",
    "\n",
    "train_prices = train_df['Price'].values\n",
    "test_prices = test_df['Price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b3d0b",
   "metadata": {},
   "source": [
    "## Section 2: Implement Evaluation Metrics\n",
    "\n",
    "Define comprehensive evaluation metrics for time series forecasts:\n",
    "- **MAE**: Mean Absolute Error - average magnitude of errors\n",
    "- **RMSE**: Root Mean Squared Error - penalizes large errors  \n",
    "- **MAPE**: Mean Absolute Percentage Error - scale-independent\n",
    "- **SMAPE**: Symmetric MAPE - fixes MAPE issues with near-zero values\n",
    "- **ME/MPE**: Mean Error/Percentage Error - measures bias\n",
    "- **MDA**: Mean Directional Accuracy - useful for trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d361ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual, forecast):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics for time series forecast\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    actual : array-like\n",
    "        Actual values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with all calculated metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    actual = np.asarray(actual)\n",
    "    forecast = np.asarray(forecast)\n",
    "    errors = actual - forecast\n",
    "    \n",
    "    # 1. Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    \n",
    "    # 2. Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean(errors ** 2))\n",
    "    \n",
    "    # 3. Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs(errors / actual)) * 100\n",
    "    \n",
    "    # 4. Symmetric MAPE (SMAPE)\n",
    "    smape = np.mean(2 * np.abs(errors) / (np.abs(actual) + np.abs(forecast))) * 100\n",
    "    \n",
    "    # 5. Mean Error (ME) - bias\n",
    "    me = np.mean(errors)\n",
    "    \n",
    "    # 6. Mean Percentage Error (MPE)\n",
    "    mpe = np.mean(errors / actual) * 100\n",
    "    \n",
    "    # 7. Mean Directional Accuracy (MDA)\n",
    "    actual_direction = np.diff(actual) > 0\n",
    "    forecast_direction = np.diff(forecast) > 0\n",
    "    mda = np.mean(actual_direction == forecast_direction) * 100\n",
    "    \n",
    "    # 8. Theil's U Statistic (normalized vs naive)\n",
    "    naive_forecast = actual[:-1]  # Last value carried forward\n",
    "    theil_u = np.sqrt(np.sum((actual[1:] - forecast[1:]) ** 2) / np.sum((actual[1:] - naive_forecast) ** 2))\n",
    "    \n",
    "    # 9. Error statistics\n",
    "    error_std = np.std(errors)\n",
    "    error_min = np.min(errors)\n",
    "    error_max = np.max(errors)\n",
    "    \n",
    "    # 10. Directional Accuracy (simple direction match)\n",
    "    da = mda  # Same as MDA\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'SMAPE': smape,\n",
    "        'ME': me,\n",
    "        'MPE': mpe,\n",
    "        'MDA': mda,\n",
    "        'Theil_U': theil_u,\n",
    "        'Error_Std': error_std,\n",
    "        'Error_Min': error_min,\n",
    "        'Error_Max': error_max,\n",
    "        'DA': da\n",
    "    }\n",
    "\n",
    "print(\"✓ Metrics calculation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b1799",
   "metadata": {},
   "source": [
    "## Section 3: Fit Multiple Forecasting Models\n",
    "\n",
    "Compare performance across different model types:\n",
    "- **ARIMA(0,1,0)**: Random walk (baseline)\n",
    "- **ARIMA(1,1,0)**: AR component\n",
    "- **ARIMA(0,1,1)**: MA component\n",
    "- **Naive**: Last value carried forward\n",
    "- **Seasonal Naive**: 252-day annual baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting forecasting models...\\n\")\n",
    "\n",
    "forecasts = {}\n",
    "\n",
    "# 1. ARIMA(0,1,0) - Random walk\n",
    "try:\n",
    "    model_arima010 = ARIMA(train_prices, order=(0,1,0))\n",
    "    model_arima010_fit = model_arima010.fit()\n",
    "    forecasts['ARIMA(0,1,0)'] = model_arima010_fit.forecast(steps=len(test_prices))\n",
    "    print(\"✓ ARIMA(0,1,0) fitted\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ARIMA(0,1,0) failed: {str(e)[:50]}\")\n",
    "\n",
    "# 2. ARIMA(1,1,0) - AR component\n",
    "try:\n",
    "    model_arima110 = ARIMA(train_prices, order=(1,1,0))\n",
    "    model_arima110_fit = model_arima110.fit()\n",
    "    forecasts['ARIMA(1,1,0)'] = model_arima110_fit.forecast(steps=len(test_prices))\n",
    "    print(\"✓ ARIMA(1,1,0) fitted\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ARIMA(1,1,0) failed: {str(e)[:50]}\")\n",
    "\n",
    "# 3. ARIMA(0,1,1) - MA component\n",
    "try:\n",
    "    model_arima011 = ARIMA(train_prices, order=(0,1,1))\n",
    "    model_arima011_fit = model_arima011.fit()\n",
    "    forecasts['ARIMA(0,1,1)'] = model_arima011_fit.forecast(steps=len(test_prices))\n",
    "    print(\"✓ ARIMA(0,1,1) fitted\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ARIMA(0,1,1) failed: {str(e)[:50]}\")\n",
    "\n",
    "# 4. Naive forecast (last value)\n",
    "forecasts['Naive'] = np.full(len(test_prices), train_prices[-1])\n",
    "print(\"✓ Naive fitted (last value)\")\n",
    "\n",
    "# 5. Seasonal Naive (252-day annual)\n",
    "seasonal_period = 252\n",
    "if len(train_prices) >= seasonal_period:\n",
    "    seasonal_naive_forecast = train_prices[-seasonal_period:seasonal_period]\n",
    "    # Repeat to match test length\n",
    "    seasonal_naive = np.tile(seasonal_naive_forecast, int(np.ceil(len(test_prices) / seasonal_period)))\n",
    "    forecasts['Seasonal Naive'] = seasonal_naive[:len(test_prices)]\n",
    "    print(f\"✓ Seasonal Naive fitted (period={seasonal_period})\")\n",
    "\n",
    "print(f\"\\n✓ All models fitted. Total models: {len(forecasts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29c7c8",
   "metadata": {},
   "source": [
    "## Section 4: Calculate Metrics for All Models\n",
    "\n",
    "Evaluate each forecast using all implemented metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4067231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, forecast in forecasts.items():\n",
    "    metrics = calculate_metrics(test_prices, forecast)\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "\n",
    "# Format for display\n",
    "metrics_display = metrics_df.copy()\n",
    "metrics_display['MAE'] = metrics_display['MAE'].apply(lambda x: f\"{x:.2f}\")\n",
    "metrics_display['RMSE'] = metrics_display['RMSE'].apply(lambda x: f\"{x:.2f}\")\n",
    "metrics_display['MAPE'] = metrics_display['MAPE'].apply(lambda x: f\"{x:.2f}%\")\n",
    "metrics_display['SMAPE'] = metrics_display['SMAPE'].apply(lambda x: f\"{x:.2f}%\")\n",
    "metrics_display['ME'] = metrics_display['ME'].apply(lambda x: f\"{x:.2f}\")\n",
    "metrics_display['MPE'] = metrics_display['MPE'].apply(lambda x: f\"{x:.2f}%\")\n",
    "metrics_display['MDA'] = metrics_display['MDA'].apply(lambda x: f\"{x:.2f}%\")\n",
    "metrics_display['DA'] = metrics_display['DA'].apply(lambda x: f\"{x:.2f}%\")\n",
    "metrics_display['Theil_U'] = metrics_display['Theil_U'].apply(lambda x: f\"{x:.4f}\")\n",
    "metrics_display['Error_Std'] = metrics_display['Error_Std'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"METRICS COMPARISON TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(metrics_display.to_string())\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765c8d3",
   "metadata": {},
   "source": [
    "## Section 5: Error Distribution Analysis\n",
    "\n",
    "Analyze properties of forecast errors: distribution, skewness, kurtosis, and bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ERROR DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for model_name, forecast in forecasts.items():\n",
    "    errors = test_prices - forecast\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean error (bias):     {np.mean(errors):8.2f}\")\n",
    "    print(f\"  Std dev of errors:     {np.std(errors):8.2f}\")\n",
    "    print(f\"  Skewness:              {stats.skew(errors):8.4f}\")\n",
    "    print(f\"  Kurtosis:              {stats.kurtosis(errors):8.4f}\")\n",
    "    print(f\"  Min error (overest):   {np.min(errors):8.2f}\")\n",
    "    print(f\"  Max error (underest):  {np.max(errors):8.2f}\")\n",
    "    \n",
    "    # Error bounds\n",
    "    mean_err = np.mean(errors)\n",
    "    std_err = np.std(errors)\n",
    "    bound_1sigma = (np.mean(np.abs(errors - mean_err) <= 1 * std_err)) * 100\n",
    "    bound_2sigma = (np.mean(np.abs(errors - mean_err) <= 2 * std_err)) * 100\n",
    "    \n",
    "    print(f\"  Errors within ±1σ:     {bound_1sigma:6.1f}%\")\n",
    "    print(f\"  Errors within ±2σ:     {bound_2sigma:6.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2bb5b",
   "metadata": {},
   "source": [
    "## Section 6: Comprehensive Visualizations\n",
    "\n",
    "Create interactive plots comparing forecasts, errors, and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafac6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Forecast vs Actual',\n",
    "        'MAE/RMSE Comparison',\n",
    "        'Forecast Errors Over Time',\n",
    "        'Error Distribution (MAE)',\n",
    "        'Directional Accuracy',\n",
    "        'MAPE/SMAPE Comparison'\n",
    "    ),\n",
    "    specs=[[{}, {}], [{}, {}], [{}, {}]],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.10\n",
    ")\n",
    "\n",
    "# Convert test dates for x-axis\n",
    "test_dates = test_df['Date'].values\n",
    "\n",
    "# 1. Forecast vs Actual (top left)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=test_dates, y=test_prices, name='Actual', \n",
    "               line=dict(color='black', width=2), mode='lines'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for (model_name, forecast), color in zip(forecasts.items(), colors):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=test_dates, y=forecast, name=model_name,\n",
    "                   line=dict(color=color, width=1, dash='dash'), mode='lines'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. MAE/RMSE Comparison (top right)\n",
    "mae_values = [all_metrics[m]['MAE'] for m in forecasts.keys()]\n",
    "rmse_values = [all_metrics[m]['RMSE'] for m in forecasts.keys()]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(forecasts.keys()), y=mae_values, name='MAE',\n",
    "           marker=dict(color='skyblue')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(forecasts.keys()), y=rmse_values, name='RMSE',\n",
    "           marker=dict(color='salmon')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Errors Over Time (middle left)\n",
    "for model_name, forecast in forecasts.items():\n",
    "    errors = test_prices - forecast\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=test_dates, y=errors, name=f'{model_name} Error',\n",
    "                   mode='markers', marker=dict(size=3)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Error Distribution (middle right) - for first model\n",
    "first_model = list(forecasts.keys())[0]\n",
    "errors_first = test_prices - forecasts[first_model]\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=errors_first, name=f'{first_model} Error Dist',\n",
    "                marker=dict(color='lightblue'), nbinsx=30),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Directional Accuracy (bottom left)\n",
    "mda_values = [all_metrics[m]['MDA'] for m in forecasts.keys()]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(forecasts.keys()), y=mda_values, name='MDA',\n",
    "           marker=dict(color='lightgreen')),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. MAPE/SMAPE Comparison (bottom right)\n",
    "mape_values = [all_metrics[m]['MAPE'] for m in forecasts.keys()]\n",
    "smape_values = [all_metrics[m]['SMAPE'] for m in forecasts.keys()]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(forecasts.keys()), y=mape_values, name='MAPE',\n",
    "           marker=dict(color='wheat')),\n",
    "    row=3, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(forecasts.keys()), y=smape_values, name='SMAPE',\n",
    "           marker=dict(color='plum')),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Model\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Error ($)\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Error ($)\", row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Error ($)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Model\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"MDA (%)\", row=3, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Model\", row=3, col=2)\n",
    "fig.update_yaxes(title_text=\"Error (%)\", row=3, col=2)\n",
    "\n",
    "fig.update_layout(height=1200, width=1400, showlegend=True, hovermode='x unified')\n",
    "fig.write_html(\"evaluation_metrics.html\")\n",
    "print(\"\\n✓ Saved interactive visualization: evaluation_metrics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223ec85",
   "metadata": {},
   "source": [
    "## Section 7: Financial Context Analysis\n",
    "\n",
    "Interpret metrics in financial context - implications for trading, risk management, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed financial analysis\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINANCIAL CONTEXT ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Find best model by each metric\n",
    "print(\"\\nBest Model by Criterion:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "metrics_to_check = {\n",
    "    'MAE': 'Lower is better (average absolute error)',\n",
    "    'RMSE': 'Lower is better (penalizes large errors)',\n",
    "    'MAPE': 'Lower is better (percentage error)',\n",
    "    'MDA': 'Higher is better (directional accuracy)',\n",
    "    'Theil_U': 'Lower is better (vs naive baseline)'\n",
    "}\n",
    "\n",
    "for metric, description in metrics_to_check.items():\n",
    "    if metric in ['MDA']:  # Higher is better\n",
    "        best_model = max(all_metrics, key=lambda x: all_metrics[x][metric])\n",
    "    else:  # Lower is better\n",
    "        best_model = min(all_metrics, key=lambda x: all_metrics[x][metric])\n",
    "    \n",
    "    value = all_metrics[best_model][metric]\n",
    "    print(f\"\\n{metric:15s}: {best_model:20s} ({value:8.2f})\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "# Trading implications\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"TRADING IMPLICATIONS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for model_name, forecast in forecasts.items():\n",
    "    errors = test_prices - forecast\n",
    "    mda = all_metrics[model_name]['MDA']\n",
    "    mae = all_metrics[model_name]['MAE']\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Direction accuracy: {mda:.1f}%\")\n",
    "    \n",
    "    if mda > 55:\n",
    "        print(f\"    → Better than random (50%) - potentially profitable on directional signals\")\n",
    "    elif mda > 50:\n",
    "        print(f\"    → Slightly better than random\")\n",
    "    else:\n",
    "        print(f\"    → Worse than random - avoid using for directional trading\")\n",
    "    \n",
    "    # PnL simulation\n",
    "    actual_direction = np.diff(test_prices) > 0\n",
    "    forecast_direction = np.diff(forecast) > 0\n",
    "    correct_directions = actual_direction == forecast_direction\n",
    "    \n",
    "    # Assume $1000 per correct direction, -$500 per wrong direction\n",
    "    pnl = np.sum(correct_directions * 1000) - np.sum(~correct_directions * 500)\n",
    "    win_rate = np.mean(correct_directions) * 100\n",
    "    \n",
    "    print(f\"  Win rate: {win_rate:.1f}%\")\n",
    "    print(f\"  Simulated PnL (±$1000/$500): ${pnl:,.0f}\")\n",
    "    print(f\"  Avg error magnitude (MAE): ${mae:.2f}\")\n",
    "\n",
    "# Risk analysis\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RISK ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for model_name, forecast in forecasts.items():\n",
    "    errors = test_prices - forecast\n",
    "    max_error = np.max(np.abs(errors))\n",
    "    percentile_95 = np.percentile(np.abs(errors), 95)\n",
    "    percentile_99 = np.percentile(np.abs(errors), 99)\n",
    "    \n",
    "    avg_price = np.mean(test_prices)\n",
    "    max_error_pct = (max_error / avg_price) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Max error:        ${max_error:8.2f} ({max_error_pct:5.1f}% of avg price)\")\n",
    "    print(f\"  95th percentile:  ${percentile_95:8.2f}\")\n",
    "    print(f\"  99th percentile:  ${percentile_99:8.2f}\")\n",
    "    \n",
    "    if max_error > 20:\n",
    "        print(f\"  Risk level: HIGH (errors can exceed $20)\")\n",
    "    elif max_error > 10:\n",
    "        print(f\"  Risk level: MODERATE (errors can exceed $10)\")\n",
    "    else:\n",
    "        print(f\"  Risk level: LOW (errors typically under $10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde9196",
   "metadata": {},
   "source": [
    "## Section 8: Summary and Recommendations\n",
    "\n",
    "Key insights and best practices for model selection and metric interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"KEY INSIGHTS AND BEST PRACTICES\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "insights = \"\"\"\n",
    "1. **Metric Selection Matters**\n",
    "   - Different metrics emphasize different aspects of forecast quality\n",
    "   - MAE: Robust, interpretable average error\n",
    "   - RMSE: Penalizes large errors (outlier sensitive)\n",
    "   - MAPE: Scale-independent percentage error\n",
    "   - MDA: Directional accuracy for trading\n",
    "\n",
    "2. **No Single Best Metric**\n",
    "   - Use complementary metrics (MAE + RMSE)\n",
    "   - Always compare to baseline (naive forecast)\n",
    "   - Consider domain-specific requirements\n",
    "\n",
    "3. **Financial Context**\n",
    "   - For trading: MDA (directional accuracy) most important\n",
    "   - For risk management: RMSE and max error matter\n",
    "   - For reporting: MAPE is most interpretable to stakeholders\n",
    "\n",
    "4. **Common Pitfalls**\n",
    "   - Relying on MAPE alone (fails on near-zero values)\n",
    "   - Ignoring outliers (not all large errors are equal)\n",
    "   - Comparing RMSE across different scales\n",
    "   - Forgetting to compare to naive baseline\n",
    "\n",
    "5. **Best Practices**\n",
    "   - Report multiple metrics in tables\n",
    "   - Visualize forecasts vs actual\n",
    "   - Show error distributions (histograms, Q-Q plots)\n",
    "   - Include confidence intervals around forecasts\n",
    "   - Document assumptions and limitations\n",
    "\"\"\"\n",
    "\n",
    "print(insights)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"METRIC SELECTION FRAMEWORK FOR GOLD PRICE FORECASTING\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "framework = \"\"\"\n",
    "For Portfolio Management:\n",
    "  → Use RMSE (emphasizes large deviations)\n",
    "  → Monitor error bounds (±1σ, ±2σ)\n",
    "  → Focus on tail risk (max error)\n",
    "\n",
    "For Trading Signals:\n",
    "  → Use MDA (directional accuracy)\n",
    "  → Need MDA > 55% for edge\n",
    "  → Combine with risk management rules\n",
    "\n",
    "For Internal Reporting:\n",
    "  → Use MAE (easy to explain: \"$X average error\")\n",
    "  → Use MAPE (percentage: \"2% average error\")\n",
    "  → Show metrics table and visualization\n",
    "\n",
    "For Stakeholder Communication:\n",
    "  → Use MAPE (\"Average error is 2.3%\")\n",
    "  → Use forecast vs actual plot\n",
    "  → Highlight comparison to naive baseline\n",
    "\"\"\"\n",
    "\n",
    "print(framework)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"✓ Day 19: Model Evaluation Metrics Complete!\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56742cbc",
   "metadata": {},
   "source": [
    "## Section 1: Load and Prepare Data\n",
    "\n",
    "Load gold price data from Yahoo Finance, clean the data, and create train/test split for model evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
